{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ef44bb-53c4-4d72-b725-05ae320b5381",
   "metadata": {},
   "source": [
    "# Welcome to TinyML part 2: Data preparation, modelling, training, and evaluation\n",
    "\n",
    " This is an interactive Jupyter notebook where you can run the Python code that we have prepared interactively.\n",
    " Below you will find cells that are either code or description.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec38bdc-9417-4446-bdba-2a5f4cdf711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules\n",
    "\n",
    "Import libraries that we need to prepare data, work with a model, train and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cade59e-813d-4674-8fca-31b69f84e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "\n",
    "from utils.calc_mem import calc_mem\n",
    "from preprocess import preprocess, WINDOW_SIZE, SPECTRUM_SIZE, SPECTRUM_MEAN, SPECTRUM_STD, SAMPLE_RATE, FRAME_SIZE, FRAME_STRIDE\n",
    "from utils.export_tflite import write_model_h_file, write_model_c_file\n",
    "from utils.plots import plot_dataset, plot_learning_curves, plot_convolution_filters, plot_first_positive_window, plot_predictions_vs_labels\n",
    "\n",
    "# Minimize TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191eb472-f9cb-436c-81a2-bf7c9cd23375",
   "metadata": {},
   "source": [
    "### Load and preprocess data\n",
    "\n",
    "This code loads and preprocesses the data and label files.\n",
    "\n",
    "**Training data format**\n",
    "\n",
    "Training data should be placed in the `Data` folder. Each recording should provide two files:\n",
    "\n",
    "* A .wav file with 16 bit mono audio in 16 kHz. Its file name should be `audio_<id>.wav`, where `<id>` can be any unique string that identifies the recording.\n",
    "* A .txt file with labels indicating the starting and ending times of each bell sound in the recording. Each row contains a starting time in seconds, the ending time in seconds and a label name, separated by tab characters. This conforms to Audacity's label file format. Its file name should be `label_<id>.txt`, where `<id>` is the recording identifier.\n",
    "\n",
    "**Signal processing**\n",
    "\n",
    "The preprocessing of a single recording takes place in the `preprocess` function in `preprocess.py`. After loading the .wav file into a numpy array, the audio is split into frames of size `FRAME_SIZE` (currently 256 audio samples corresponding to 16 ms), and the following signal processing is performed on each frame:\n",
    "\n",
    "* Subtract the frame average to remove any constant bias in the audio frame.\n",
    "* Apply a Hamming window - common practice before FFT to avoid spectral leakage.\n",
    "* Perform a real FFT (Fast Fourier Transform), which produces a spectral frame with 129 frequency bins. Frequency bin `i` contains the magnitude at frequency `i * SAMPLE_RATE / FRAME_SIZE`, so for example, bin 40 contains the magnitude at frequency 40 * 16000 Hz / 256 = 2500 Hz.\n",
    "* Remove bins above 3000 Hz (bin 48), since they contain little relevant information and we want to keep input data small.\n",
    "* Apply log(1 + x) to compress high magnitude values and obtain an even spread of values.\n",
    "* Normalize the data to ensure it has approximately mean 0 and standard deviation 1, which helps neural network training.\n",
    "\n",
    "**Sliding windows**\n",
    "\n",
    "The sequence of all spectral frames constitute the complete spectrogram for the audio file. However, the classifier that we're about to train can neither work on a single spectral frame, because 16 ms of sound is too little to make classifications on, nor can it work on the entire file, because it's too big. Therefore, we use a sliding window over the spectrogram to cut out small spectrograms of length `WINDOW_SIZE` (currently 32 frames, corresponding to 256 ms). This window slides over the full spectrogram with a stride of `WINDOW_STRIDE` (currently 8 frames, corresponding to 64 ms). In other words, the windows are overlapping. Each such window will constitute an *input matrix*, or *feature matrix*, of size 32x48. Our classifier will take one such window and classify whether it contains a bell sound or not. With over 60 minutes of audio data, our entire data set contains more than 30,000 such windows.\n",
    "\n",
    "**Labelling windows from time intervals**\n",
    "\n",
    "Each window must have a label - `0` or `1` - to indicate to the training algorithm whether the window contains the bell sound or not. However, our label files contain time intervals in seconds, for example:\n",
    "\n",
    "```\n",
    "25.29\t26.29\tDing\n",
    "70.578\t71.578\tDing\n",
    "115.866\t116.866\tDing\n",
    "...\n",
    "```\n",
    "\n",
    "The window labeling algoritm at the end of the `preprocess` function assigns the label `1` if the window is completely within a time interval in the label file. For example:\n",
    "\n",
    "* The window with index 395, starting at 25.28 s and ending at 25.536 s, is assigned label `0`, because it's not completely within the interval 25.29 - 26.29.\n",
    "* The window with index 396, starting at 25.334 s and ending at 25.6 s, is assigned label `1`, because it's completely within the interval.\n",
    "\n",
    "**Return shapes**\n",
    "\n",
    "The `preprocess` function returns one numpy array `x` of shape `(n, 32, 48)`, containing all *n* windows (aka small spectrograms aka feature matrices) and one numpy array `y` with *n* labels for the recording. Since we have multiple recording files, these individual `x` and `y` for each file are gathered in a list and finally concatenated into one big `x` of `(N, 32, 48)` and `y` of shape `(N)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3017b-bd8e-42dd-8ed6-28b9272cb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data_dir = '../Data/'\n",
    "x_files = []\n",
    "y_files = []\n",
    "for c_file in os.listdir('../Data/'):\n",
    "    if c_file.startswith('audio_') and c_file.endswith('.wav'):\n",
    "        recording_id = c_file[6:-4]\n",
    "        label_file = 'labels_' + recording_id + '.txt'\n",
    "        print('Preprocessing ' + c_file + ' and ' + label_file)\n",
    "        x_file, y_file = preprocess(data_dir + c_file, data_dir + label_file)\n",
    "        x_files.append(x_file)\n",
    "        y_files.append(y_file)\n",
    "\n",
    "# Concatenate files into feature and label arrays\n",
    "x = np.concatenate(x_files)  # Shape: (number of windows, WINDOW_SIZE, SPECTRUM_SIZE) = (number of windows, 32, 48)\n",
    "y = np.concatenate(y_files)  # Shape: (number of windows, 1)\n",
    "\n",
    "# Plot the spectrogram of the entire dataset with labels underneath\n",
    "plot_dataset(x, y, block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training, validation and test sets and determine class weights\n",
    "\n",
    "**Splitting data**\n",
    "\n",
    "A common beginner mistake is to use all data for training. This is bad for two reasons:\n",
    "\n",
    "* We have no means of controlling overfitting. *Overfitting* is when the model is trained \"too well\" on the data set, so it adapts to noise and outliers in the data set. It's typically caused by training a too big model on too little data. The consequence is that the model works extremely well on the training data, but when it's tested on new unheard or unseen data, it performs very badly. Overfitting is a common problem in machine learning scenarios, especially with smaller data sets.\n",
    "* We have no way of evaluating the model's accuracy. Computing the accuracy on the same data that the model was trained on will give a very optimistic result.\n",
    "\n",
    "The solution to split the entire data set into three subsets:\n",
    "\n",
    "* **Training set**:  Used for training the model, i.e. optimizing the model parameters.\n",
    "* **Validation set**: Used for *early stopping*, which means stopping the training when no improvement can be seen on the validation set. This is a common method to prevent overfitting.\n",
    "* **Test set**: Used for computing a reliable accuracy.\n",
    "\n",
    "**Class weights**\n",
    "\n",
    "Another common beginner mistake is to have an unbalanced data set, which means to not have approximately the same number of examples for all classes, and not doing anything about it. This is clearly the case in our project, where the bell sounds only 1 out of 45 seconds. In fact, we have we have approximately 51 times more `0` examples (windows with no bell sound) than `1` examples (windows with bell sound). The problem with unbalanced data is that the training will favorize the overrepresented class, since it then appears more accurate when evaluated on the data set. Even a classifer that always outputs `0` regardless input would appear quite accurate with ~98% accuracy on our data set.\n",
    "\n",
    "The best solution to this problem is to complement with more data from the underrepresented class (in our case, more bell sound data) to ensure that the data set is balanced. If that's not possible, one can compensate by providing *class weights* to TensorFlow, telling it to weigh the examples labeled with a certain class higher. Since we have about 51 times more `0` exaples than `1` examples, we give class `1` a weight that is 51 times higher than that for class `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training, validation and test sets\n",
    "x_train, x_val, x_test = np.split(x, [int(.6 * len(x)), int(.8 * len(x))])\n",
    "y_train, y_val, y_test = np.split(y, [int(.6 * len(y)), int(.8 * len(y))])\n",
    "\n",
    "# Determine class weights\n",
    "num_positives = np.sum(y)\n",
    "num_negatives = len(y) - num_positives\n",
    "ratio = num_negatives / num_positives\n",
    "class_weights = {0: 1 / np.sqrt(ratio), 1: np.sqrt(ratio)}  # Divide by sqrt(ratio) to make losses comparable\n",
    "print('Class weights:', class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and compile the model\n",
    "\n",
    "We choose a model consisting of four layers:\n",
    "\n",
    "* A `Conv1D` layer with 8 filters of size 3 to detect raw patterns over 3 frames (48 ms).\n",
    "* Another `Conv1D` layer with 8 filters of size 3 to detect combined patterns over 5 frames (80 ms).\n",
    "* A `Flatten` layer to turn convolution output to a vector; required for Dense.\n",
    "* A `Dense` layer with a single neuron to summarize into prediction output.\n",
    "\n",
    "With `model.compile()`, the model is combined with:\n",
    "\n",
    "* A *loss function* to be minimized, in this case `binary_crossentropy`, which is the default choice for binary classifiers.\n",
    "* An *optimization algorithm* for minimizing the loss, in this case `adam`, which is typically the default choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile model\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Conv1D(8, 3, activation='relu', input_shape=(WINDOW_SIZE, SPECTRUM_SIZE)))  # Output shape (30, 8)\n",
    "model.add(Conv1D(8, 3, activation='relu'))  # Output shape (28, 8)\n",
    "model.add(Flatten())  # Output shape (224,)\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output shape (1)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Training runs in iterations called *epochs*. For each epoch, the following happens:\n",
    "\n",
    "* The optimizer runs over the entire training data set one batch (128 windows) at a time. For each batch:\n",
    "    * The loss function is computed on the batch.\n",
    "    * The gradient of the loss function is computed using backpropagation.\n",
    "    * The parameters are updated using the gradient.\n",
    "* The callbacks are called:\n",
    "    * `early_stopping`: If the validation loss has not improved for 16 epochs, training stops.\n",
    "    * `model_checkpoint`: If the validation loss is the lowest so far, the model and parameters are saved to a file.\n",
    "\n",
    "The learning curve plot shows the value of the loss function over the epochs for both the training set and the validation set. The learning curve provides valuable information about how well the training went. In particular:\n",
    "\n",
    "* The training loss should decrease quite smoothly. If not, training is unstable and hyper-parameters like learning rate should be tuned, and the data should be (better) shuffled.\n",
    "* The validation loss should also decrease, but typically not as smoothly. It may also increase slightly by the end of the training, which indicates that the model is starting to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with early stopping and class weights; save best model\n",
    "print('Training model...')\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=16)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_val, y_val), class_weight=class_weights,\n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves(model, block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some filters from the trained model\n",
    "\n",
    "Among other model parameters, the training has generated some convolution filters. It's instructuve to inspect these filters: Since the windows of class `1` (the ones containing the bell sound) have high magnitudes at certain frequencies, there will typically one or more filters that likewise have high values at these frequencies. You should see the amplified frequencies as yellow horizontal bands in the spectrogram window of a class `1` example, and some yellow cells in the same rows in some of the convolution filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first window that has a positive label\n",
    "plot_first_positive_window(x, y, block=False)\n",
    "\n",
    "# Plot the 8 convolution filters of the first layer\n",
    "plot_convolution_filters(8, model.layers[0], block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Inspecting evaluation metrics and plots from the trained model is important for two reasons:\n",
    "\n",
    "* To see if the model is good enough or if there are any problems.\n",
    "* To figure out what to do if it's not good enough.\n",
    "\n",
    "Among evaluation metrics, we will look at the following:\n",
    "\n",
    "* Validation loss: The value of the loss function computed over the validation set. This is the number to look at to compare different models: the lower loss, the better. Since there is some randomness in the training, simply rerunning the training may produce a better model. Notice that it's not enough to just rerun the training cell in this notebook, because training will just continue from where it was stopped - the model must be rebuilt from scratch, so restart from Build and compile the model.\n",
    "* Validation accuracy: The classification accuracy computed over the validation set. This number is misleading on unbalanced data sets like ours and should not be used.\n",
    "* Test loss: The value of the loss function computed over the test set. This number should not be considerably higher than the validation loss. A higher test loss may indicate that the model is overfit.\n",
    "* Test accuracy: The classification accuracy computed over the test set. Again this is misleading on unbalanced data sets, but with a balanced data set, this number is the most intuitive evaluation metric.\n",
    "\n",
    "The next four numbers count the number of true and false positive and negative classifications on the test set. The sum of the four numbers is equal to the number of examples in the test set. Other commonly used evaluation metrics like precision and recall can be derived from these numbers.\n",
    "\n",
    "* True positives: The number of correct `1` classifcations.\n",
    "* True negatives: The number of correct `0` classifactions.\n",
    "* False positives: The number of `1` classifications on windows that were labeled `0`, indicating that the classifier detected a bell sound when it wasn't actually there.\n",
    "* False negatives: The number of `0` classifications on windows that were labeled `1`, indicating that the classifier didn't detect a bell sound when it was actually there.\n",
    "\n",
    "It's important to note that these numbers count window classifications, which is not the same as bell sound instances, since multiple window classifications happen during a single bell sound instance. Most of the false positives and negatives are typically just that the classifier started detecting or stopped detecting the bell sound slightly too early or too late, but that's not a problem in the actual application. To get a better sense of actually problematic misclassifications, it's better to look at a plot of the predictions versus the labels over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = keras.models.load_model('model.h5')\n",
    "\n",
    "# Evaluate model on validation and test sets\n",
    "val_loss, val_accuracy = model.evaluate(x_val, y_val)\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print()\n",
    "print('Validation loss:     %.4f' % val_loss)\n",
    "print('Validation accuracy: %.4f' % val_accuracy)\n",
    "print('Test loss:           %.4f' % test_loss)\n",
    "print('Test accuracy:       %.4f' % test_accuracy)\n",
    "\n",
    "# Print confusion matrix\n",
    "y_pred_bool = np.round(y_pred)\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "for i in range(len(y_pred_bool)):\n",
    "    confusion_matrix[int(y_test[i]), int(y_pred_bool[i, 0])] += 1\n",
    "print('True positives:     ', int(confusion_matrix[1, 1]))\n",
    "print('True negatives:     ', int(confusion_matrix[0, 0]))\n",
    "print('False positives:    ', int(confusion_matrix[0, 1]))\n",
    "print('False negatives:    ', int(confusion_matrix[1, 0]))\n",
    "\n",
    "# Plot predictions vs labels\n",
    "plot_predictions_vs_labels(y_pred, y_test, block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model to C\n",
    "\n",
    "Finally, the model is converted to a TensorFlow Lite model and then exported to C code, so it can be loaded in the ESP32 application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Lite model\n",
    "print('Converting to TensorFlow Lite model...')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Export TensorFlow Lite model to C source files\n",
    "print('Exporting TensorFlow Lite model to C source files...')\n",
    "defines = {\"SAMPLE_RATE\": SAMPLE_RATE,\n",
    "           \"FRAME_SIZE\": FRAME_SIZE,\n",
    "           \"FRAME_STRIDE\": FRAME_STRIDE,\n",
    "           \"WINDOW_SIZE\": WINDOW_SIZE,\n",
    "           \"SPECTRUM_SIZE\": SPECTRUM_SIZE,\n",
    "           \"SPECTRUM_MEAN\": SPECTRUM_MEAN,\n",
    "           \"SPECTRUM_STD\": SPECTRUM_STD}\n",
    "write_model_h_file(\"../ESP-32/main/model.h\", defines)\n",
    "write_model_c_file('../ESP-32/main/model.c', tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TensorFlow Lite model and print memory\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "calc_mem(\"model.tflite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
